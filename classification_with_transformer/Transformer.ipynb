{"cells":[{"cell_type":"code","source":["parameters = \"\"\"\n","#### parameters\n","-----------------\n","vocab_size (vocabulary size of training data) =\n","embedding_dim (d_model) =\n","context_length =\n","num_heads =\n","dff (feedforward network units) =\n","num_layers (num of encoders/decoders) =\n","\n","# optional\n","dropout_rate\n","\"\"\"\n","print(parameters)"],"metadata":{"id":"D_fWToFpY-_c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1694687860538,"user_tz":-330,"elapsed":1171,"user":{"displayName":"Madhusudhana","userId":"07612415316378441544"}},"outputId":"9eeef438-9ccf-470a-cd79-118de97271a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","#### parameters\n","-----------------\n","vocab_size (vocabulary size of training data) =\n","embedding_dim (d_model) =\n","context_length =\n","num_heads =\n","dff (feedforward network units) =\n","num_layers (num of encoders/decoders) =\n","\n","# optional\n","dropout_rate\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oZEUcUZw5itE"},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import logging"]},{"cell_type":"code","source":["logging.basicConfig(filename='transformer.log', encoding='utf-8', level=logging.DEBUG)"],"metadata":{"id":"LbfHf_wYaRqS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hgiv1pwPODBI"},"outputs":[],"source":["def positional_encoding(context_length,embeding_dim):\n","\n","    # embeding_dim = 4  # DIMENSIONS OF THE MODEL\n","    # context_length = 4 # CONTEXT LENGTH OF THE MODEL, means it can handle 4 words from the input\n","\n","    # TOTAL POSITIONS\n","    positions = np.arange(context_length)[:,np.newaxis]\n","\n","    embeding_dim = embeding_dim/2\n","\n","    positions = np.arange(context_length)[:, np.newaxis]     # (seq, 1)\n","    embeding_dim = np.arange(embeding_dim)[np.newaxis, :]/embeding_dim   # (1, depth)\n","\n","    angle_rates = 1 / (10000**embeding_dim)         # (1, depth)\n","    angle_rads = positions * angle_rates      # (pos, depth)\n","\n","    first_half = np.sin(angle_rads)\n","    second_half = np.cos(angle_rads)\n","\n","    positional_encoding = np.concatenate([first_half, second_half],axis=1)\n","\n","    return tf.cast(positional_encoding, dtype=tf.float32)"]},{"cell_type":"code","source":["class PositionalEncoding(tf.keras.layers.Layer):\n","\n","    def __init__(self, vocab_size=10000, context_length=2064, d_model=512):\n","\n","        self.d_model = d_model\n","        self.context_length = context_length\n","        self.vocab_size = vocab_size\n","\n","        super(PositionalEncoding, self).__init__()\n","\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True)\n","        self.positional_encoding = positional_encoding(context_length, d_model)\n","\n","    def call(self, x):\n","        print(f\"Dimensions of the each word = {self.d_model}\\n\")\n","        print(f\"Input words shape {x.shape}\\n\")\n","\n","        assert len(x.shape) == 2\n","\n","        # LENGTH OF THE SENTECES, IN REAL TIME THE LENGTH OF SENTECES WILL GET DIFFER\n","        length = tf.shape(x)[1]\n","\n","        # GETTING EMBEDDINGS\n","        embeds = self.embedding(x)\n","        print(f\"embedding shape = {embeds.shape}\\n\")\n","\n","        # OPTIONAL\n","        embeds *= tf.math.sqrt(tf.cast(512, tf.float32))\n","\n","        # GETTING POSITIONS BASED ON THE LENGTH OF THE SENTENCE(WORDS)\n","        pos_en = self.positional_encoding[tf.newaxis, :length, :]\n","        print(f\"positional encoding shape = {pos_en.shape}\\n\")\n","\n","\n","        # COMBINING POSITIONAL ENCODING AND EMBEDDINGS\n","        op = embeds + pos_en\n","        print(f\"Output shape = {op.shape}\")\n","        return op"],"metadata":{"id":"a1F9fP62Xoja"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pe = PositionalEncoding(10000, 2048, 1024)"],"metadata":{"id":"reo04XG_Hg6s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pe(tf.Variable([[2],[3]]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vekN7EQZHwrc","executionInfo":{"status":"ok","timestamp":1700993795335,"user_tz":-330,"elapsed":340,"user":{"displayName":"Madhusudhan Reddy Gone","userId":"07612415316378441544"}},"outputId":"4e6b9979-75a0-43b6-a91c-62349068b578"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dimensions of the each word = 1024\n","\n","Input words shape (2, 1)\n","\n","embedding shape = (2, 1, 1024)\n","\n","positional encoding shape = (1, 1, 1024)\n","\n","Output shape = (2, 1, 1024)\n"]},{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(2, 1, 1024), dtype=float32, numpy=\n","array([[[ 0.69965667, -0.10972746, -0.5721705 , ...,  1.9067972 ,\n","         -0.00867653,  1.207585  ]],\n","\n","       [[-0.389815  ,  0.2653671 ,  1.1063471 , ..., -0.01849329,\n","         -0.09468436,  1.6995642 ]]], dtype=float32)>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Base Attentions, core components will have in this base attentions\n","class BaseAttention(tf.keras.layers.Layer):\n","    def __init__(self, **kwargs):\n","        super().__init__()\n","        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n","        self.add = tf.keras.layers.Add()\n","        self.layernorm = tf.keras.layers.LayerNormalization()"],"metadata":{"id":"E98IWRksXoKx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GlobalAttention(BaseAttention):\n","    def call(self, input):\n","        attention_output = self.mha(\n","            query=input,\n","            value=input,\n","            key=input,\n","        )\n","        addition = self.add([input, attention_output])\n","        return self.layernorm(addition)"],"metadata":{"id":"yVaYYIezob9W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CrossAttention(BaseAttention):\n","    \"\"\"\n","    At Decoder after casual attention\n","    \"\"\"\n","    def call(self, input, context):\n","        attention_output, attention_score = self.mha(\n","            query=input,\n","            value=context,\n","            key=context,\n","            return_attention_scores = True\n","        )\n","\n","        self.attention_score = attention_score\n","\n","        addition = self.add([input, attention_output])\n","        return self.layernorm(addition)"],"metadata":{"id":"QV6LoYLcVnyx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CasualAttention(BaseAttention):\n","    \"\"\"\n","    At Decoder starting layer\n","    \"\"\"\n","    def call(self, input):\n","        attention_output = self.mha(\n","            query=input,\n","            value=input,\n","            key=input,\n","            use_casual_mask = True\n","        )\n","        addition = self.add([input, attention_output])\n","        return self.layernorm(addition)"],"metadata":{"id":"GCUy0sz4Wvnr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","class FeedForward(tf.keras.layers.Layer):\n","    def __init__(self, d_model, dff, dropout_rate=0.1):\n","        super().__init__()\n","        self.seq = tf.keras.Sequential([\n","            tf.keras.layers.Dense(dff, activation = 'relu'),\n","            tf.keras.layers.Dense(d_model),\n","            tf.keras.layers.Dropout(dropout_rate),\n","        ])\n","        self.add = tf.keras.layers.Add()\n","        self.layernorm = tf.keras.layers.LayerNormalization()\n","\n","    def call(self, input):\n","        seq = self.seq(input)\n","        add = self.add([input, seq])\n","        return self.layernorm(add)"],"metadata":{"id":"C9zzjKK5W21d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EncoderLayer(tf.keras.layers.Layer):\n","    def __init__(self, num_heads, key_dim, dff, dropout_rate=0.1):\n","        super().__init__()\n","        self.self_attention = GlobalAttention(\n","            num_heads=num_heads,\n","            key_dim=key_dim,\n","            dropout= dropout_rate,\n","        )\n","        self.ff = FeedForward(key_dim, dff)\n","\n","    def call(self, input):\n","        x = self.self_attention(input)\n","        return self.ff(x)"],"metadata":{"id":"odiiqvU8cHXv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Encoder(tf.keras.layers.Layer):\n","\n","    def __init__(self,num_layers, d_model, num_heads, dff, vocab_size, context_length, dropout_rate=0.1):\n","        super().__init__()\n","        self.num_layers = num_layers\n","\n","        self.pos_embedding = PositionalEncoding(vocab_size,context_length, d_model)\n","        self.en_layers = [\n","            EncoderLayer(\n","                num_heads , d_model, dff, dropout_rate\n","            ) for layer in range(num_layers)\n","        ]\n","        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n","\n","    def call(self, input):\n","        print(f\"Encoder Input Shape = {input.shape}\")\n","        x = self.pos_embedding(input)\n","        print(f\"Endoder Embedding Shape = {x.shape}\")\n","        for layer in range(self.num_layers):\n","            x = self.en_layers[layer](x)\n","\n","        print(f\"All Endoder Output Shape = {x.shape}\")\n","        return x"],"metadata":{"id":"kIsU1xKshzlo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Prepare Data\n","\n","num_words = 20000 # total words\n","max_length = 200 # length of the sentence\n","ds = tf.keras.datasets.imdb.load_data(num_words=num_words)"],"metadata":{"id":"8SjInSipmKJY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_x, train_y, val_x, val_y = ds[0][0], ds[0][1], ds[1][0], ds[1][1]"],"metadata":{"id":"77Kz9bG6nLtT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(train_x), \"Training sequences\")\n","print(len(val_x), \"Validation sequences\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7yR3PTDyaqK4","executionInfo":{"status":"ok","timestamp":1694687865569,"user_tz":-330,"elapsed":9,"user":{"displayName":"Madhusudhana","userId":"07612415316378441544"}},"outputId":"cda1f716-2c43-4e45-ccca-bbd58351bff2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["25000 Training sequences\n","25000 Validation sequences\n"]}]},{"cell_type":"code","source":["train_x = tf.keras.utils.pad_sequences(train_x,maxlen=max_length)\n","val_x = tf.keras.utils.pad_sequences(val_x,maxlen=max_length)"],"metadata":{"id":"ZFiAMfxcazlf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(train_x.shape, \"Training sequences\")\n","print(val_x.shape, \"Validation sequences\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pg2ejXG3bROx","executionInfo":{"status":"ok","timestamp":1694687867061,"user_tz":-330,"elapsed":33,"user":{"displayName":"Madhusudhana","userId":"07612415316378441544"}},"outputId":"7d221ec0-c72a-479e-e359-cf7557ba1bf7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(25000, 200) Training sequences\n","(25000, 200) Validation sequences\n"]}]},{"cell_type":"code","source":["transformer_encoder = Encoder(num_layers=1,d_model=32,num_heads=2,dff=32,vocab_size=num_words,context_length=max_length)"],"metadata":{"id":"NZAksL00bWp4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inputs = tf.keras.layers.Input(shape=(max_length,))\n","x = transformer_encoder(inputs)\n","x = tf.keras.layers.GlobalAveragePooling1D()(x)\n","x = tf.keras.layers.Dropout(0.1)(x)\n","x = tf.keras.layers.Dense(20, activation=\"relu\")(x)\n","x = tf.keras.layers.Dropout(0.1)(x)\n","outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n","model = tf.keras.Model(inputs=inputs, outputs=outputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i1mE2Rk3b_gh","executionInfo":{"status":"ok","timestamp":1694687867062,"user_tz":-330,"elapsed":28,"user":{"displayName":"Madhusudhana","userId":"07612415316378441544"}},"outputId":"70fd4cea-0e94-4f33-f2f3-2883aba36709"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoder Input Shape = (None, 200)\n","Dimensions of the each word = 32\n","\n","Input words shape (None, 200)\n","\n","embedding shape = (None, 200, 32)\n","\n","positional encoding shape = (1, 200, 32)\n","\n","Output shape = (None, 200, 32)\n","Endoder Embedding Shape = (None, 200, 32)\n","All Endoder Output Shape = (None, 200, 32)\n"]}]},{"cell_type":"code","source":["model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n","history = model.fit(\n","    train_x, train_y, batch_size=64, epochs=3, validation_data=(val_x, val_y)\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xi2egCtCdb6b","outputId":"85203a01-738a-45d3-dd4f-923cf9c9035e","executionInfo":{"status":"ok","timestamp":1694688010734,"user_tz":-330,"elapsed":143687,"user":{"displayName":"Madhusudhana","userId":"07612415316378441544"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3\n","Encoder Input Shape = (None, 200)\n","Dimensions of the each word = 32\n","\n","Input words shape (None, 200)\n","\n","embedding shape = (None, 200, 32)\n","\n","positional encoding shape = (1, 200, 32)\n","\n","Output shape = (None, 200, 32)\n","Endoder Embedding Shape = (None, 200, 32)\n","All Endoder Output Shape = (None, 200, 32)\n","Encoder Input Shape = (None, 200)\n","Dimensions of the each word = 32\n","\n","Input words shape (None, 200)\n","\n","embedding shape = (None, 200, 32)\n","\n","positional encoding shape = (1, 200, 32)\n","\n","Output shape = (None, 200, 32)\n","Endoder Embedding Shape = (None, 200, 32)\n","All Endoder Output Shape = (None, 200, 32)\n","391/391 [==============================] - ETA: 0s - loss: 0.3937 - accuracy: 0.8096Encoder Input Shape = (None, 200)\n","Dimensions of the each word = 32\n","\n","Input words shape (None, 200)\n","\n","embedding shape = (None, 200, 32)\n","\n","positional encoding shape = (1, 200, 32)\n","\n","Output shape = (None, 200, 32)\n","Endoder Embedding Shape = (None, 200, 32)\n","All Endoder Output Shape = (None, 200, 32)\n","391/391 [==============================] - 65s 155ms/step - loss: 0.3937 - accuracy: 0.8096 - val_loss: 0.3011 - val_accuracy: 0.8770\n","Epoch 2/3\n","391/391 [==============================] - 23s 59ms/step - loss: 0.1908 - accuracy: 0.9267 - val_loss: 0.3082 - val_accuracy: 0.8682\n","Epoch 3/3\n","391/391 [==============================] - 19s 48ms/step - loss: 0.1149 - accuracy: 0.9599 - val_loss: 0.4645 - val_accuracy: 0.8486\n"]}]},{"cell_type":"code","source":["print(train_x[0][np.newaxis,:].shape)\n","prediction = model.predict(train_x[0][np.newaxis,:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uHWXtCsA0fgU","executionInfo":{"status":"ok","timestamp":1694688010743,"user_tz":-330,"elapsed":29,"user":{"displayName":"Madhusudhana","userId":"07612415316378441544"}},"outputId":"2f2e8a6b-12c4-4d71-d147-f61e7255bf9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 200)\n","Encoder Input Shape = (None, 200)\n","Dimensions of the each word = 32\n","\n","Input words shape (None, 200)\n","\n","embedding shape = (None, 200, 32)\n","\n","positional encoding shape = (1, 200, 32)\n","\n","Output shape = (None, 200, 32)\n","Endoder Embedding Shape = (None, 200, 32)\n","All Endoder Output Shape = (None, 200, 32)\n","1/1 [==============================] - 0s 222ms/step\n"]}]},{"cell_type":"code","source":["prediction"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hmo5P3ZoJ3DG","executionInfo":{"status":"ok","timestamp":1694688010743,"user_tz":-330,"elapsed":25,"user":{"displayName":"Madhusudhana","userId":"07612415316378441544"}},"outputId":"c47ce072-3722-46fd-a7ca-ab557f7bc81e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.00169578, 0.99830425]], dtype=float32)"]},"metadata":{},"execution_count":117}]},{"cell_type":"code","source":["train_y[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dx800tJLKHpq","executionInfo":{"status":"ok","timestamp":1694688010744,"user_tz":-330,"elapsed":22,"user":{"displayName":"Madhusudhana","userId":"07612415316378441544"}},"outputId":"1b07cbf3-45a6-4e14-a622-cad502834de4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":118}]},{"cell_type":"code","source":["model.save(\"/content/drive/MyDrive/Models/transformer_imdb_classification\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qx5u0HZMKLQs","executionInfo":{"status":"ok","timestamp":1694688116094,"user_tz":-330,"elapsed":59008,"user":{"displayName":"Madhusudhana","userId":"07612415316378441544"}},"outputId":"0d898306-42b3-4dd0-cc6b-1f321048de8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoder Input Shape = (None, 200)\n","Dimensions of the each word = 32\n","\n","Input words shape (None, 200)\n","\n","embedding shape = (None, 200, 32)\n","\n","positional encoding shape = (1, 200, 32)\n","\n","Output shape = (None, 200, 32)\n","Endoder Embedding Shape = (None, 200, 32)\n","All Endoder Output Shape = (None, 200, 32)\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Skipping full serialization of Keras layer <keras.src.layers.regularization.dropout.Dropout object at 0x7e92f4522110>, because it is not built.\n"]},{"output_type":"stream","name":"stdout","text":["Encoder Input Shape = (None, 200)\n","Dimensions of the each word = 32\n","\n","Input words shape (None, 200)\n","\n","embedding shape = (None, 200, 32)\n","\n","positional encoding shape = (1, 200, 32)\n","\n","Output shape = (None, 200, 32)\n","Endoder Embedding Shape = (None, 200, 32)\n","All Endoder Output Shape = (None, 200, 32)\n","Encoder Input Shape = (None, 200)\n","Dimensions of the each word = 32\n","\n","Input words shape (None, 200)\n","\n","embedding shape = (None, 200, 32)\n","\n","positional encoding shape = (1, 200, 32)\n","\n","Output shape = (None, 200, 32)\n","Endoder Embedding Shape = (None, 200, 32)\n","All Endoder Output Shape = (None, 200, 32)\n","Encoder Input Shape = (None, 200)\n","Dimensions of the each word = 32\n","\n","Input words shape (None, 200)\n","\n","embedding shape = (None, 200, 32)\n","\n","positional encoding shape = (1, 200, 32)\n","\n","Output shape = (None, 200, 32)\n","Endoder Embedding Shape = (None, 200, 32)\n","All Endoder Output Shape = (None, 200, 32)\n","Encoder Input Shape = (None, 200)\n","Dimensions of the each word = 32\n","\n","Input words shape (None, 200)\n","\n","embedding shape = (None, 200, 32)\n","\n","positional encoding shape = (1, 200, 32)\n","\n","Output shape = (None, 200, 32)\n","Endoder Embedding Shape = (None, 200, 32)\n","All Endoder Output Shape = (None, 200, 32)\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"4DGb3QXCR5Po"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1VmAgmR9bl5g9_5jBSVno5bNM_Ow81QaG","authorship_tag":"ABX9TyNlnXz8fY6sb9CECP64MKHg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}